{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "A matrix decomposition is a way of reducing a matrix into its constituent parts. An approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself.\n",
    "\n",
    "An analogy for matrix decomposition is the factoring of numbers, such as the factoring of 10 into 2 * 5. For this reason, matrix decomposition is also known as matrix factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two widely used matrix decopositions methods are the LU matrix decomposition and QR matrix decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Decomposition\n",
    "LU Decomposition is for square matrices and decomposes a matrix into L and U components. \n",
    "\n",
    "A = L.U or A = LU; \n",
    "where A is the square matrix that we wish to decompose, L is the lower triangle matrix and U is the upper triangle matrix.\n",
    "The factors L and U are triangular matrices and the factorization comes from elimination is A = LU.  \n",
    "\n",
    "The LU decomposition is found using an iterative numerical process and can fail for those matrices that cannot be decomposed or decomposed easily. A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with partial pivoting.\n",
    "\n",
    "A = L.U.P\n",
    "\n",
    "The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional P matrix specifies a way to permuute the result or return the result to the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]\n",
      " [5 6 7]]\n",
      "\r\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "\r\n",
      "[[1.   0.   0.  ]\n",
      " [0.2  1.   0.  ]\n",
      " [0.8  0.25 1.  ]\n",
      " [0.6  0.5  0.  ]\n",
      " [0.4  0.75 0.5 ]]\n",
      "\r\n",
      "[[5.0000000e+00 6.0000000e+00 7.0000000e+00]\n",
      " [0.0000000e+00 8.0000000e-01 1.6000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 8.8817842e-16]]\n",
      "\r\n",
      "[[1. 2. 3.]\n",
      " [2. 3. 4.]\n",
      " [3. 4. 5.]\n",
      " [4. 5. 6.]\n",
      " [5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import lu\n",
    "\n",
    "A = array([[1,2,3],\n",
    "          [2,3,4],\n",
    "          [3,4,5],\n",
    "          [4,5,6],\n",
    "          [5,6,7]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "P,L,U = lu(A)\n",
    "\n",
    "print(P)\n",
    "print(\"\\r\")\n",
    "print(L)\n",
    "print(\"\\r\")\n",
    "print(U)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "# reconstructing the matrix\n",
    "B = P.dot(L).dot(U)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "The QR decomposition is for n*m matrices and decomposes a matrix into Q and R components.\n",
    "\n",
    "A = Q.R or A = QR\n",
    "; where A is the matrix we wish to decompose , Q is a matrix with the size of m * m, and R is an upper triangle matrix with the size m*n.\n",
    "The QR decomposition is found using an iterative numerical method that can fail for those matrices that cannot be decomposed, or decomposed easily. Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, although is not limited to square matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns the Q and R matrices with smaller or reduced dimensions that is more economical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]\n",
      " [4 5 6]\n",
      " [5 6 7]]\n",
      "\r\n",
      "[[-0.13483997 -0.76277007 -0.44927365 -0.33396384 -0.29431505]\n",
      " [-0.26967994 -0.47673129  0.83432661  0.02485082  0.05728476]\n",
      " [-0.40451992 -0.19069252 -0.30269368  0.32007421  0.77841444]\n",
      " [-0.53935989  0.09534626 -0.10049789  0.62115447 -0.55142297]\n",
      " [-0.67419986  0.38138504  0.0181386  -0.63211566  0.01003882]]\n",
      "\r\n",
      "[[-7.41619849e+00 -9.43879807e+00 -1.14613977e+01]\n",
      " [ 0.00000000e+00 -9.53462589e-01 -1.90692518e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -9.96498610e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\r\n",
      "[[1. 2. 3.]\n",
      " [2. 3. 4.]\n",
      " [3. 4. 5.]\n",
      " [4. 5. 6.]\n",
      " [5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import qr\n",
    "\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "# factorize\n",
    "Q,R = qr(A, 'complete')\n",
    "print(Q)\n",
    "print(\"\\r\")\n",
    "print(R)\n",
    "print(\"\\r\")\n",
    "B = Q.dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition\n",
    "It is for square symmetric matrices where all values are greater than zero, so called positive definite matrices. The decomposition is defined as follows:\n",
    "\n",
    "A = L.L_transform\n",
    "; where L is the lower triangular matrix and L_transform is the transpose of L.\n",
    "\n",
    "It can be written as the product of the upper triangular matrix,\n",
    "A = U_transform.U\n",
    "; where U is the upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1]\n",
      " [1 2 1 1 1]\n",
      " [1 1 2 1 1]\n",
      " [1 1 1 2 1]\n",
      " [1 1 1 1 2]]\n",
      "\n",
      "[[1.41421356 0.         0.         0.         0.        ]\n",
      " [0.70710678 1.22474487 0.         0.         0.        ]\n",
      " [0.70710678 0.40824829 1.15470054 0.         0.        ]\n",
      " [0.70710678 0.40824829 0.28867513 1.11803399 0.        ]\n",
      " [0.70710678 0.40824829 0.28867513 0.2236068  1.09544512]]\n",
      "\n",
      "[[2. 1. 1. 1. 1.]\n",
      " [1. 2. 1. 1. 1.]\n",
      " [1. 1. 2. 1. 1.]\n",
      " [1. 1. 1. 2. 1.]\n",
      " [1. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import cholesky\n",
    "\n",
    "A = array([[2,1,1,1,1],\n",
    "          [1,2,1,1,1],\n",
    "          [1,1,2,1,1],\n",
    "          [1,1,1,2,1],\n",
    "          [1,1,1,1,2]])\n",
    "\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "L = cholesky(A)\n",
    "print(L)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "B = L.dot(L.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen decomposition of a matrix\n",
    "Eigen decomposition is the most used type of matrix decomposition that decomposes a matrix into eigen vectors and eigen values. This decomposition also plays a role in methods used in machine learning, such as the Principal Component Analysis(PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector is an eigenvector of a matrix if it satisfies the following equation.\n",
    "\n",
    "A . v = lambda(lowercase) . v.\n",
    "\n",
    "This is called the eigenvalue equation, where A is the parent square matrix that we are decomposing, v is the eigenvector of the matrix and lambda(lowercase greek letter) represents the eigenvalue scalar.\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent  matrix. Not all square matrices can be decomposed into eigenvectors and eigenvalues, and some can only be decomposed in a way that requires complex numbers. The parent matrix can be represente as a product of the eigenvectors and eigenvalues.\n",
    "\n",
    "A = Q . lambda(uppercase) . Q_transpose\n",
    "; Q is the matrix comprised of the eigenvectors, lambda(uppercase) represents the diagonal matrix comprised of the eigen values, Q_transpose is the transpose of the matrix comprised of eigenvectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decomposing a matrix into their eigenvalues and eigenvectors can help us analyze certain properties of a matrix, much as decomposing an integer into its prime factors.\n",
    "\n",
    "The decomposition of a matrix does not result in a compression of the matrix, instead, it breaks it down into constituent parts to make certain operations on the matrix easier to perform. \n",
    "\n",
    "Almost all vectors change direction when multipied by A. Certain exceptional vectors 'x' are in the same direction as A.x. Those are the 'eigenvectors'. Multiply an eigenvector by A, and the vector is lambda(lowercase) times the original x. The eigenvalue lambda(lowercase) tells whether the special vector x is stretched or shrunk or reversed or left unchanged when multipied by A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]]\n",
      "\r\n",
      "[ 1.29282032e+01 -9.28203230e-01  9.22462701e-16]\n",
      "\r\n",
      "[[-0.28932501 -0.80222426  0.40824829]\n",
      " [-0.53988782 -0.10747767 -0.81649658]\n",
      " [-0.79045062  0.58726892  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "A = array([[1,2,3],\n",
    "          [3,4,5],\n",
    "          [5,6,7]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print(\"\\r\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm an eigenvalue and eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.74045251  -6.9797794  -10.21910629]\n",
      "\r\n",
      "[ -3.74045251  -6.9797794  -10.21910629]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "# Factorize\n",
    "values, vectors = eig(A)\n",
    "\n",
    "# confirm first eigenvector\n",
    "B = A.dot(vectors[:,0])\n",
    "print(B)\n",
    "print(\"\\r\")\n",
    "\n",
    "C = vectors[:,0] * values[0]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import diag\n",
    "from numpy.linalg import inv, eig\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]]\n",
      "\r\n",
      "[[1. 2. 3.]\n",
      " [3. 4. 5.]\n",
      " [5. 6. 7.]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "values, vectors = eig(A)\n",
    "\n",
    "# create matrix from eigenvectors\n",
    "Q = vectors\n",
    "\n",
    "# create inverse of eigenvectors matrix\n",
    "R = inv(Q)\n",
    "\n",
    "# create diagonal matrix from from eigenvalues\n",
    "L = diag(values)\n",
    "\n",
    "# reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "Singular Value Decomposition or SVD is a matrix decomposition for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. \n",
    "\n",
    "A = U.sigma(uppercase).V_transform\n",
    "; where A is the real n * m matrix that we wish to decompose, U is an m * m matrix, sigma(uppercase greek letter sigma) is an m * n diagonal matrix, and V_transform is the transpose of an n * n matrix where T is a superscipt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal values in the sigma(uppercase) matrix are known as the singular values of the original matrix of A. The columns of the U matrix are called the left-singular matrix of A, and the columns of V are called the right-singular vectors of A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is calculated by using iterative numerical methods. The SVD provides another way to factorize a matrix into singular vectors and singular values. It allows us to discover the same kind of information as the eigendecomposition. However, SVD is more generally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is widely used in calculation of other matrix operations , such as matrix inverse, also as a data reduction method in machine learning. It can also be used in Least sqaures regression, image compression and denoising data.\n",
    "\n",
    "Applying SVD to a matrix is like looking at it with an X-ray vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [3 4 5]]\n",
      "\r\n",
      "[[-0.38508979  0.82767094  0.40824829]\n",
      " [-0.55951021  0.14241368 -0.81649658]\n",
      " [-0.73393063 -0.54284358  0.40824829]]\n",
      "\r\n",
      "[9.62347538e+00 6.23475383e-01 2.98457310e-16]\n",
      "\r\n",
      "[[-0.38508979 -0.55951021 -0.73393063]\n",
      " [-0.82767094 -0.14241368  0.54284358]\n",
      " [-0.40824829  0.81649658 -0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "\n",
    "A = array([[1,2,3],\n",
    "          [2,3,4],\n",
    "          [3,4,5]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "U, s, V = svd(A)\n",
    "print(U)\n",
    "print(\"\\r\")\n",
    "print(s)\n",
    "print(\"\\r\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructing the matrix\n",
    "The original matrix can be reconstructed from the U, s and V_transform elements. However, the U,s and V_transform elements returned from the svd() function cannot be multipied directly. The s vector must be converted into a diagonal matrix using diag() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructing rectangular matrix from svd()\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\r\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "A = array([[1,2],\n",
    "          [3,4],\n",
    "          [5,6]])\n",
    "print(A)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "U,s,V = svd(A)\n",
    "\n",
    "# create m * n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "\n",
    "# populate Sigma with n * n diagonal matrix\n",
    "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
    "\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above case with the Sigma diagonal happens only with the case where m and n are not equal. The diagonal matrix can be used directly when reconstructing a square matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\r\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import diag\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# define matrix\n",
    "A = array([[1,2,3],\n",
    "          [4,5,6],\n",
    "          [7,8,9]])\n",
    "print(A)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "U,s,V = svd(A)\n",
    "\n",
    "# create n * n diagonal matrix\n",
    "Sigma = diag(s)\n",
    "\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudoinverse\n",
    "The pseudoinverse is the generalization of the matrix inverse for square matrices for square matrices to rectangular matrices where the number of rows and columns are not equal, also called the Moore-Penrose inverse after two independent discoverers of the method or the Generalized Inverse.\n",
    "\n",
    "Matrix Inversion is not defined for non-square matrices. When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n",
    "\n",
    "The pseudoinverse is denoted as A_plus(plus is the superscript) and A is the matrix being inverted.\n",
    "\n",
    "A_plus = V . D_plus . U_transform\n",
    "; where A_plus is the pseudoinverse, D_plus is the pseudoinverse of the diagonal matrix sigma and V_transform is the transpose of V. We get U and V from the SVD operation.\n",
    "The D_plus is the pseudoinverse of the diagonal matrix sigma. The pseudoinverse provides one way of solving the linear regression equation, specifically when there are more rows than there are columns, which is often the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n",
      "\r\n",
      "[[-1.00000000e+00 -5.00000000e-01  1.60786705e-15  5.00000000e-01]\n",
      " [ 8.50000000e-01  4.50000000e-01  5.00000000e-02 -3.50000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import pinv\n",
    "\n",
    "A = array([[1,2],\n",
    "          [3,4],\n",
    "          [5,6],\n",
    "          [7,8]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# calculating pseudoinverse\n",
    "B = pinv(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the pseudoinverse manually via the SVD and compare the results to the pinv() function.\n",
    "\n",
    "1. Calculate the SVD\n",
    "2. Calculate the reciprocal of each value in s array.\n",
    "3. Transform the s array into a diagonal matrix with an added rows of zero to make it rectangular.\n",
    "4. Calculate the pseudoinverse from the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]\n",
      " [0.9 1. ]]\n",
      "\r\n",
      "[[-8.  -5.  -2.   1.   4. ]\n",
      " [ 7.   4.5  2.  -0.5 -3. ]]\n"
     ]
    }
   ],
   "source": [
    "# Pseudoinvere via SVD\n",
    "\n",
    "from numpy import array\n",
    "from numpy.linalg import svd\n",
    "from numpy import zeros\n",
    "from numpy import diag\n",
    "\n",
    "A = array([[0.1,0.2],\n",
    "          [0.3,0.4],\n",
    "          [0.5,0.6],\n",
    "          [0.7,0.8],\n",
    "          [0.9,1.0]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "U,s,V = svd(A)\n",
    "\n",
    "# reciprocal of s \n",
    "d = 1.0/s\n",
    "\n",
    "# create an m * nD matrix\n",
    "D = zeros(A.shape)\n",
    "\n",
    "# populate D with n * n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = diag(d)\n",
    "\n",
    "# calculate pseudoinverse\n",
    "B = V.T.dot(D.T).dot(U.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    "Primarily used in Machine Learning applications as a part of preprocessing and feature engineering. Data with large number of features need to be reduced to a lower number of features that are most relevant to the prediction problem. \n",
    "\n",
    "The result is a matrix with a lower rank that is said to approximate the original matrix. This can be done using SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [11 12 13 14 15]\n",
      " [21 22 23 24 25]]\n",
      "\r\n",
      "[[ 1.  2.  3.  4.  5.]\n",
      " [11. 12. 13. 14. 15.]\n",
      " [21. 22. 23. 24. 25.]]\n",
      "\r\n",
      "[[ -6.93431959   2.62967904]\n",
      " [-29.2269437    0.88643206]\n",
      " [-51.51956782  -0.85681493]]\n",
      "\r\n",
      "[[ -6.93431959   2.62967904]\n",
      " [-29.2269437    0.88643206]\n",
      " [-51.51956782  -0.85681493]]\n"
     ]
    }
   ],
   "source": [
    "# data reduction with svd\n",
    "A = array([[1,2,3,4,5],\n",
    "          [11,12,13,14,15],\n",
    "          [21,22,23,24,25]])\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# Factorize\n",
    "U,s,V = svd(A)\n",
    "\n",
    "# create an m * n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "\n",
    "# populate Sigma with n * n diagonal matrix\n",
    "Sigma[:A.shape[0], :A.shape[0]] = diag(s)\n",
    "\n",
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "V = V[:n_elements,:]\n",
    "\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "# transform\n",
    "T = U.dot(Sigma)\n",
    "print(T)\n",
    "\n",
    "print(\"\\r\")\n",
    "\n",
    "T = A.dot(V.T)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [11 12 13 14 15]\n",
      " [21 22 23 24 25]]\n",
      "\r\n",
      "[[ 6.93431959  2.62967904]\n",
      " [29.2269437   0.88643206]\n",
      " [51.51956782 -0.85681493]]\n"
     ]
    }
   ],
   "source": [
    "# svd data reduction in scikit-learn\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(A)\n",
    "print(\"\\r\")\n",
    "\n",
    "# create transform\n",
    "svd = TruncatedSVD(n_components = 2)\n",
    "svd.fit(A)\n",
    "result = svd.transform(A)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
